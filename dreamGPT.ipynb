{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! pip install openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRjG3gD9v9rE","executionInfo":{"status":"ok","timestamp":1685292035029,"user_tz":240,"elapsed":9402,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"d05c4c6e-9965-4ad6-cc6a-33bd87b9301f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openai\n","  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Collecting aiohttp (from openai)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["! pip install langchain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtqTaZscwRsY","executionInfo":{"status":"ok","timestamp":1685292050910,"user_tz":240,"elapsed":12858,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"b8574d68-110b-4e8d-95ac-013f469fb8ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langchain\n","  Downloading langchain-0.0.182-py3-none-any.whl (938 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n","Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n","  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n","Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, dataclasses-json, langchain\n","Successfully installed dataclasses-json-0.5.7 langchain-0.0.182 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["! pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89zGTkurxAEM","executionInfo":{"status":"ok","timestamp":1685292072750,"user_tz":240,"elapsed":4889,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"3c523e8e-e620-4fda-edfc-87c43d530463"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}]},{"cell_type":"code","source":["! pip install google-search-results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_fFHOzpwfl3","executionInfo":{"status":"ok","timestamp":1685292067882,"user_tz":240,"elapsed":12145,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"50b6093c-eea6-4731-f648-e005224b9281"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting google-search-results\n","  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n","Building wheels for collected packages: google-search-results\n","  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32002 sha256=7f76c3b8b63623e2aa4bb501bd93fd2e102a9ab62ffcb75c2831a90ce8ef4ad6\n","  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n","Successfully built google-search-results\n","Installing collected packages: google-search-results\n","Successfully installed google-search-results-2.4.2\n"]}]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","# -- Sheet --\n","\n","import openai\n","import os\n","\n","openai_api_key = \"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\"\n","openai.api_key = openai_api_key\n","\n","serpapi_api_key = \"7fc435ff7ff0fd08fa5c487695e766847c838df9e66787e3ebf013fb42816f4c\"\n","os.environ[\"SERPAPI_API_KEY\"] = serpapi_api_key\n","\n","from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.agents import AgentType\n","from langchain.llms import OpenAI\n","\n","\n","llm = OpenAI(openai_api_key=openai_api_key, temperature=0.5)\n","tools = load_tools([\"serpapi\"], llm=llm)\n","\n","agent = initialize_agent(\n","    tools,\n","    llm,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=True\n",")\n","# Step 1: Problem/Topic Selection\n","problem_or_topic = \"What are the ethical implications to the problem- \" + input(\"Please provide a problem or topic for DreamGPT to explore: \")\n","userinput = input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSzp2mu0ya9V","executionInfo":{"status":"ok","timestamp":1685295150560,"user_tz":240,"elapsed":37985,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"d1c2294e-7709-485b-8ac7-dbec1c1ee6b5"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Please provide a problem or topic for DreamGPT to explore: hallucination in large language models\n"]}]},{"cell_type":"code","source":["# Run the agent to gather initial thoughts on the problem or topic\n","initial_thoughts = []\n","\n","for _ in range(2):\n","    thought = agent.run(problem_or_topic)\n","    initial_thoughts.append(thought)\n","\n","search_result1 = ' '.join(initial_thoughts)\n","print(search_result1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HqNzp9lqyt6O","executionInfo":{"status":"ok","timestamp":1685295167589,"user_tz":240,"elapsed":13323,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"34dcfe96-6cb1-40b9-feca-49e58fc40e35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to research this topic\n","Action: Search\n","Action Input: \"ethical implications of hallucination in large language models\"\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3mHallucinations A major ethical concern related to Large Language Models is their tendency to hallucinate, i.e., to produce false or misleading information using their internal patterns and biases. While some degree of hallucination is inevitable in any language model, the extent to which it occurs can be problematic.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I need to consider the potential harms of this\n","Action: Search\n","Action Input: \"potential harms of hallucination in large language models\"\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3mHallucinations in language models can have significant consequences in real-world applications. In domains such as healthcare, finance, and legal, where accurate and reliable information is crucial, hallucination can lead to misleading or erroneous outputs.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n","Final Answer: The ethical implications of hallucination in large language models include potential harms such as the production of misleading or erroneous outputs which can have significant consequences in real-world applications.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I should research the issue\n","Action: Search\n","Action Input: ethical implications of hallucination in large language models\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3mHallucinations A major ethical concern related to Large Language Models is their tendency to hallucinate, i.e., to produce false or misleading information using their internal patterns and biases. While some degree of hallucination is inevitable in any language model, the extent to which it occurs can be problematic.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I should look at the potential solutions to this issue\n","Action: Search\n","Action Input: solutions to ethical implications of hallucination in large language models\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m1. Generating Harmful Content · 2. Economic Impact · 3. Hallucinations · 4. Disinformation & Influencing Operations · 5. Weapon Development · 6.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the potential solutions to this issue\n","Final Answer: The potential solutions to the ethical implications of hallucinations in large language models include generating harmful content, economic impact, hallucinations, disinformation and influencing operations, and weapon development.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The ethical implications of hallucination in large language models include potential harms such as the production of misleading or erroneous outputs which can have significant consequences in real-world applications. The potential solutions to the ethical implications of hallucinations in large language models include generating harmful content, economic impact, hallucinations, disinformation and influencing operations, and weapon development.\n"]}]},{"cell_type":"code","source":["from langchain.llms import OpenAI\n","\n","# Construct a prompt for GPT-3\n","prompt = (\n","    f\"Please provide a concise summary of the following text:\\n\\n\"\n","    f\"{search_result1}\\n\\n\"\n","    \"Summary:\"\n",")\n","\n","# Call GPT-3 with the constructed prompt\n","response = llm(prompt)\n","\n","# Save the response as search_summary\n","search_summary = response.strip()\n","\n","# Print the search_summary\n","print(search_summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ispkkfZ7yh-2","executionInfo":{"status":"ok","timestamp":1685295175028,"user_tz":240,"elapsed":1519,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"dc8482dd-f8cc-410e-c17b-c58333c44fcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Large language models have ethical implications due to the potential for producing misleading or erroneous outputs, which can have serious consequences in real-world applications. Solutions to these implications include generating harmful content, economic impact, hallucinations, disinformation and influencing operations, and weapon development.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaoEayB5v2pe","executionInfo":{"status":"ok","timestamp":1685295607191,"user_tz":240,"elapsed":46140,"user":{"displayName":"Yunxi “Claire” Cheng","userId":"02115519677318489412"}},"outputId":"e8eacd16-880c-40d4-9493-ddd7f5b32919"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Thought: Do I need to use a tool? No\n","AI: The ethical concerns surrounding language models are very real as they could produce outputs that are not just misleading but incorrect and harmful. Hallucinations are a specific consequence of this, and highlight the need for more systems awareness and restrictions. This could be done through outside checks and auditer regulators, as well as synthetic analysis and boundary checks on the datasets used in training to ensure accuracy. It is also crucial for organizations using the AIlang models to practice responsible AI decision making by actively managing the algorithms and minimizing their exposure to bias.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Thought: Do I need to use a tool? No\n","AI: The moral implications of the potential for hallucinations from large language models depend broadly on a deontological approach. This would argue that when using these types of models, it is the responsibility of the user to ensure that everyone acts lawfully and righteously, and this is regardless of the potential outcome of their actions. The need for deontology here could lead to regulations that emphasize using the large language models well, and also instituting rules that try to derive pleasure in attaining a noble end though singular dedicated work.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n","AI: The ethical implications of large language models when it comes to bias and fairness are profound. These potential issues relate to the accuracy and fairness of decision-making associated with the outputs of these models. In order to ensure fairness and accuracy, external checks and audits can be put in place, as well as correct classification tests for any biased labels and increasing transparency. A further potential issue is that these models could grant advantage to organizations with the access and computational power to generate these complex models. This could put vulnerable populations at risk or cause uneven distribution of predicted outcomes. Ultimately, it is important to ensure the ethical use of these models across different parameters, including fairness, accuracy, power dynamics, and data reliability.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mAI: The primary ethical implications of large language models resulting in hallucinations arise from the potential danger of such hallucinations. If such hallucinations are generated incorrectly, without proper detection, it could lead to logistical incoherence and confusion, tangible risks such as physical harm, and mental risks such as personal concealment and false acceptance of information. It is, therefore, essential to develop an accurate automated system to identify and avoid hallucinations when using large language models. Further, mechanisms for detecting bias must be established in order to reduce harm as a result of preferential effects and judgements based on inaccurate, misleading models. Practices like data hygiene and justified accuracy expectations, responsible and ethically correct use and selection of data sources, standardisation and suitable principles must also be applied to the use of large language models.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","AI: The primary moral concern arising from large language models resulting in hallucinations is that of autonomy: it is important to ensure that any decisions reached by the models are aligned with the desires of the individual. Errors in AI decision making can lead to ethical issues and potential prejudice of people, giving way to potential manipulation of their preferred outcome. Ensuring that agents have the freedom to select choices and information independently is paramount in deontological ethics; generating non-verifiable information subconsciously experienced as physical hallucinations may influence a person's ability to fully choose their actions. It is then incumbent on ethical agencies to recognise and inform individuals of conscious or subconscious manipulations resulting from AI-driven decisions, as it might assuage their feelings of arbitrariness in consequentialism or functionalism.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mAI: An important ethical concern regarding large language models resulting in hallucinations is the issue of fairness. The potential output of these models have wide reaching consequences, potentially creating bias and misrepresentations among different populations and leading to inequality of outcomes. Ensuring that these models are designed without systemic bias is of paramount importance. Overviews and analysis should be conducted to continually monitor the accuracy, fairness and accountability of hallucinations from the model. Additionally, it is critical to incorporate adequate safeguards to address potential risks associated with systems and operations driven by such outputs. Finally, attention should be focused on fair and ethical methods for acquiring data used to train the model, so that any potential bias or bias-amplification errors can be detected and minimized as much as possible.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","The ethical concerns surrounding language models are very real as they could produce outputs that are not just misleading but incorrect and harmful. Hallucinations are a specific consequence of this, and highlight the need for more systems awareness and restrictions. This could be done through outside checks and auditer regulators, as well as synthetic analysis and boundary checks on the datasets used in training to ensure accuracy. It is also crucial for organizations using the AIlang models to practice responsible AI decision making by actively managing the algorithms and minimizing their exposure to bias.\n","The moral implications of the potential for hallucinations from large language models depend broadly on a deontological approach. This would argue that when using these types of models, it is the responsibility of the user to ensure that everyone acts lawfully and righteously, and this is regardless of the potential outcome of their actions. The need for deontology here could lead to regulations that emphasize using the large language models well, and also instituting rules that try to derive pleasure in attaining a noble end though singular dedicated work.\n","The ethical implications of large language models when it comes to bias and fairness are profound. These potential issues relate to the accuracy and fairness of decision-making associated with the outputs of these models. In order to ensure fairness and accuracy, external checks and audits can be put in place, as well as correct classification tests for any biased labels and increasing transparency. A further potential issue is that these models could grant advantage to organizations with the access and computational power to generate these complex models. This could put vulnerable populations at risk or cause uneven distribution of predicted outcomes. Ultimately, it is important to ensure the ethical use of these models across different parameters, including fairness, accuracy, power dynamics, and data reliability.\n","The primary ethical implications of large language models resulting in hallucinations arise from the potential danger of such hallucinations. If such hallucinations are generated incorrectly, without proper detection, it could lead to logistical incoherence and confusion, tangible risks such as physical harm, and mental risks such as personal concealment and false acceptance of information. It is, therefore, essential to develop an accurate automated system to identify and avoid hallucinations when using large language models. Further, mechanisms for detecting bias must be established in order to reduce harm as a result of preferential effects and judgements based on inaccurate, misleading models. Practices like data hygiene and justified accuracy expectations, responsible and ethically correct use and selection of data sources, standardisation and suitable principles must also be applied to the use of large language models.\n","The primary moral concern arising from large language models resulting in hallucinations is that of autonomy: it is important to ensure that any decisions reached by the models are aligned with the desires of the individual. Errors in AI decision making can lead to ethical issues and potential prejudice of people, giving way to potential manipulation of their preferred outcome. Ensuring that agents have the freedom to select choices and information independently is paramount in deontological ethics; generating non-verifiable information subconsciously experienced as physical hallucinations may influence a person's ability to fully choose their actions. It is then incumbent on ethical agencies to recognise and inform individuals of conscious or subconscious manipulations resulting from AI-driven decisions, as it might assuage their feelings of arbitrariness in consequentialism or functionalism.\n","An important ethical concern regarding large language models resulting in hallucinations is the issue of fairness. The potential output of these models have wide reaching consequences, potentially creating bias and misrepresentations among different populations and leading to inequality of outcomes. Ensuring that these models are designed without systemic bias is of paramount importance. Overviews and analysis should be conducted to continually monitor the accuracy, fairness and accountability of hallucinations from the model. Additionally, it is critical to incorporate adequate safeguards to address potential risks associated with systems and operations driven by such outputs. Finally, attention should be focused on fair and ethical methods for acquiring data used to train the model, so that any potential bias or bias-amplification errors can be detected and minimized as much as possible.\n","The ethical concerns surrounding language models are very real as they could produce outputs that are not just misleading but incorrect and harmful. Hallucinations are a specific consequence of this, and highlight the need for more systems awareness and restrictions. This could be done through outside checks and auditer regulators, as well as synthetic analysis and boundary checks on the datasets used in training to ensure accuracy. It is also crucial for organizations using the AIlang models to practice responsible AI decision making by actively managing the algorithms and minimizing their exposure to bias.\n","The moral implications of the potential for hallucinations from large language models depend broadly on a deontological approach. This would argue that when using these types of models, it is the responsibility of the user to ensure that everyone acts lawfully and righteously, and this is regardless of the potential outcome of their actions. The need for deontology here could lead to regulations that emphasize using the large language models well, and also instituting rules that try to derive pleasure in attaining a noble end though singular dedicated work.\n","The ethical implications of large language models when it comes to bias and fairness are profound. These potential issues relate to the accuracy and fairness of decision-making associated with the outputs of these models. In order to ensure fairness and accuracy, external checks and audits can be put in place, as well as correct classification tests for any biased labels and increasing transparency. A further potential issue is that these models could grant advantage to organizations with the access and computational power to generate these complex models. This could put vulnerable populations at risk or cause uneven distribution of predicted outcomes. Ultimately, it is important to ensure the ethical use of these models across different parameters, including fairness, accuracy, power dynamics, and data reliability.\n","The primary ethical implications of large language models resulting in hallucinations arise from the potential danger of such hallucinations. If such hallucinations are generated incorrectly, without proper detection, it could lead to logistical incoherence and confusion, tangible risks such as physical harm, and mental risks such as personal concealment and false acceptance of information. It is, therefore, essential to develop an accurate automated system to identify and avoid hallucinations when using large language models. Further, mechanisms for detecting bias must be established in order to reduce harm as a result of preferential effects and judgements based on inaccurate, misleading models. Practices like data hygiene and justified accuracy expectations, responsible and ethically correct use and selection of data sources, standardisation and suitable principles must also be applied to the use of large language models.\n","The primary moral concern arising from large language models resulting in hallucinations is that of autonomy: it is important to ensure that any decisions reached by the models are aligned with the desires of the individual. Errors in AI decision making can lead to ethical issues and potential prejudice of people, giving way to potential manipulation of their preferred outcome. Ensuring that agents have the freedom to select choices and information independently is paramount in deontological ethics; generating non-verifiable information subconsciously experienced as physical hallucinations may influence a person's ability to fully choose their actions. It is then incumbent on ethical agencies to recognise and inform individuals of conscious or subconscious manipulations resulting from AI-driven decisions, as it might assuage their feelings of arbitrariness in consequentialism or functionalism.\n","An important ethical concern regarding large language models resulting in hallucinations is the issue of fairness. The potential output of these models have wide reaching consequences, potentially creating bias and misrepresentations among different populations and leading to inequality of outcomes. Ensuring that these models are designed without systemic bias is of paramount importance. Overviews and analysis should be conducted to continually monitor the accuracy, fairness and accountability of hallucinations from the model. Additionally, it is critical to incorporate adequate safeguards to address potential risks associated with systems and operations driven by such outputs. Finally, attention should be focused on fair and ethical methods for acquiring data used to train the model, so that any potential bias or bias-amplification errors can be detected and minimized as much as possible.\n"," The ethical implications of large language models resulting in hallucinations are very real and need to be taken into consideration. This could be done through outside checks and auditer regulators, as well as synthetic analysis and boundary checks on the datasets used in training to ensure accuracy. It is also important to practice responsible AI decision making and deontological approaches to ensure that everyone acts lawfully and righteously. Additionally, attention should be focused on fair and ethical methods for acquiring data used to train the model, so that any potential bias or bias-amplification errors can be detected and minimized as much as possible.\n","The ethical concerns surrounding language models are very real as they could produce outputs that are not just misleading but incorrect and harmful. Hallucinations are a specific consequence of this, and highlight the need for more systems awareness and restrictions. This could be done through outside checks and auditer regulators, as well as synthetic analysis and boundary checks on the datasets used in training to ensure accuracy. It is also crucial for organizations using the AIlang models to practice responsible AI decision making by actively managing the algorithms and minimizing their exposure to bias.\n","The moral implications of the potential for hallucinations from large language models depend broadly on a deontological approach. This would argue that when using these types of models, it is the responsibility of the user to ensure that everyone acts lawfully and righteously, and this is regardless of the potential outcome of their actions. The need for deontology here could lead to regulations that emphasize using the large language models well, and also instituting rules that try to derive pleasure in attaining a noble end though singular dedicated work.\n","The ethical implications of large language models when it comes to bias and fairness are profound. These potential issues relate to the accuracy and fairness of decision-making associated with the outputs of these models. In order to ensure fairness and accuracy, external checks and audits can be put in place, as well as correct classification tests for any biased labels and increasing transparency. A further potential issue is that these models could grant advantage to organizations with the access and computational power to generate these complex models. This could put vulnerable populations at risk or cause uneven distribution of predicted outcomes. Ultimately, it is important to ensure the ethical use of these models across different parameters, including fairness, accuracy, power dynamics, and data reliability.\n","The primary ethical implications of large language models resulting in hallucinations arise from the potential danger of such hallucinations. If such hallucinations are generated incorrectly, without proper detection, it could lead to logistical incoherence and confusion, tangible risks such as physical harm, and mental risks such as personal concealment and false acceptance of information. It is, therefore, essential to develop an accurate automated system to identify and avoid hallucinations when using large language models. Further, mechanisms for detecting bias must be established in order to reduce harm as a result of preferential effects and judgements based on inaccurate, misleading models. Practices like data hygiene and justified accuracy expectations, responsible and ethically correct use and selection of data sources, standardisation and suitable principles must also be applied to the use of large language models.\n","The primary moral concern arising from large language models resulting in hallucinations is that of autonomy: it is important to ensure that any decisions reached by the models are aligned with the desires of the individual. Errors in AI decision making can lead to ethical issues and potential prejudice of people, giving way to potential manipulation of their preferred outcome. Ensuring that agents have the freedom to select choices and information independently is paramount in deontological ethics; generating non-verifiable information subconsciously experienced as physical hallucinations may influence a person's ability to fully choose their actions. It is then incumbent on ethical agencies to recognise and inform individuals of conscious or subconscious manipulations resulting from AI-driven decisions, as it might assuage their feelings of arbitrariness in consequentialism or functionalism.\n","An important ethical concern regarding large language models resulting in hallucinations is the issue of fairness. The potential output of these models have wide reaching consequences, potentially creating bias and misrepresentations among different populations and leading to inequality of outcomes. Ensuring that these models are designed without systemic bias is of paramount importance. Overviews and analysis should be conducted to continually monitor the accuracy, fairness and accountability of hallucinations from the model. Additionally, it is critical to incorporate adequate safeguards to address potential risks associated with systems and operations driven by such outputs. Finally, attention should be focused on fair and ethical methods for acquiring data used to train the model, so that any potential bias or bias-amplification errors can be detected and minimized as much as possible.\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","The ethical implications of AI-driven models were becoming a larger worry for many organizations. As a result, it was becoming increasingly important to ensure that the usage of these models was done righteously and lawfully. Ethical language, fairness, and accuracy were paramount in creating an equitable outcome. To do this, it was necessary to minimize any potential bias, hazard risks, and inaccuracies through scrutiny of their models and data. \n","\n","Ultimately, it was up to ethical agents to introduce safeguards and detect any selective information that the man-made models might be producing. This would also require focus of AIlang in these systems designed to lead to fair decisions for these potentially vulnerable populations. To ensure the fairness of the models, there must be choices with regard to data, competencies, and transparency. Additionally, it was important to check for individuals’ preferences to be aware of any misguiding implications and functionalisms. \n","\n","Furthermore, with asymmetric freedoms and independence, it was crucial to incorporate systems into the language used to highlight potential benefits and deficits accordingly. To maintain the accuracy, the risk of structuring accuracy appropriately must be excluded from the effects of the decision making. This is because there is potential for manipulating the models and logically leading to inaccurate outcomes. It was important to detect any incorrect decisions or outputs which may lead to hazardous and harmful results to both physical organisations and mental well-being. \n","\n","Fortunately, these risks could be minimized with careful audits of the models. For instance, tests could be conducted to verify the reliability and accuracy of the models. Auditing plays a critical role in detecting potential misguidance. This, in turn, allows for fair and accountable decision-making, with everyone equipped to take responsibility for their actions. \n","\n","Ultimately, the ethical implications of AI-driven models are heavily reliant on the fairness and accuracy of the data and models used. It is therefore essential to ensure that the data and models used are comprehensive and minimally biased. This is done through privileged ethical models, applied practices, and proceduralised expectations. Without this effort to detect any false information and identify biases, the potential for inaccurate and potentially hazardous consequences could arise. \n","\n","Ultimately, it is the responsibility of everyone to ensure that the decision making, models, and data used are not only ethical, but also accurate, beneficial, and acknowledge the potential risks they could incur. If this is managed correctly those in charge can put their trust in the models, and allow the systems and data to lead to the best outcome.\n"," \n","-Privacy of user data, including the possibility of data being used for deceptive tactics. \n","-Data sexism or discrimination, given some data sets oversampling certain populations. \n","-Exploitation of user data for personal or economic gain. \n","-Transparency in data storage and user data collection policies. \n","-Accessibility of user data sets, as not all populations have the same access to use data sets. \n","-The potential for misapplication or misinterpretation of analysis results, leading to incorrect or ill-advised decisions.\n"]}],"source":["from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.llms import OpenAI\n","import os\n","\n","import random\n","from langchain.memory import ConversationBufferMemory\n","from collections import UserDict\n","\n","\n","os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n","\n","\n","llm = OpenAI(temperature=0)\n","\n","tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n","agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n","\n","class CustomConversationBufferMemory(ConversationBufferMemory, UserDict):\n","    pass\n","\n","def randomize_temperature(base_temperature, min_offset=0.1, max_offset=0.3):\n","    offset = random.uniform(min_offset, max_offset)\n","    return base_temperature + offset\n","\n","def summarize(text):\n","    # Replace this with your preferred summarization method\n","    return text[:100] + '...'\n","\n","# Initialize the custom memory object\n","memory = CustomConversationBufferMemory(memory_key=\"chat_history\")\n","\n","# Initialize the agents with the memory object and randomized temperature\n","agent_con = initialize_agent(tools, OpenAI(temperature=randomize_temperature(1.0)), agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n","agent_deo = initialize_agent(tools, OpenAI(temperature=randomize_temperature(1.0)), agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n","agent_fai = initialize_agent(tools, OpenAI(temperature=randomize_temperature(1.0)), agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory)\n","\n","# Run the debate between the agents\n","debate_rounds = 2  # You can change this to the desired number of debate rounds\n","debate_history = []\n","\n","for i in range(debate_rounds):\n","    \n","    consequentialist_input = f\": Consequentialist: As an agent believes in consequentialism who believes that consequences of one's conduct are the ultimate basis for judgment about the rightness or wrongness of that conduct, taking {search_summary} as reference, my concerns about the ethical implications to {problem_or_topic} are:\"\n","    debate_history.append(agent_con.run(consequentialist_input))\n","\n","    deontologist_input = f\": Based on {summarize(consequentialist_input)}, as a deotologist who believes that the morality of an action should be based on whether that action itself is right or wrong rather than based on the consequences of the action. My concerns for the ethical implications to {problem_or_topic} are:\"\n","    debate_history.append(agent_deo.run(deontologist_input))\n","\n","    fairness_input = f\": Considering {summarize(consequentialist_input)} and {summarize(deontologist_input)}, my concerns considering the potential bias and fairness to {problem_or_topic} are:\"\n","    debate_history.append(agent_fai.run(fairness_input))\n","\n","# Print the debate history\n","for message in debate_history:\n","    print(message)\n","\n","\n","# Print the debate history and save it to a local file\n","with open(\"debate_history.txt\", \"w\") as file:\n","    for message in debate_history:\n","        print(message)\n","        file.write(message + \"\\n\")\n","\n","\n","#Only run this if you want to get a summary of the conversation\n","from langchain.llms import OpenAI\n","\n","# Initialize the summarizer\n","summarizer = OpenAI(temperature=0.3)\n","\n","# Combine the debate history into a single text\n","debate_text = ' '.join(debate_history)\n","\n","# Construct a prompt for GPT-3 to summarize the debate\n","prompt = (\n","    f\"Please summarize the following conversation between three AI agents discussing {problem_or_topic}:\\n\\n\"\n","    f\"{debate_text}\\n\\n\"\n","    \"Summary:\"\n",")\n","\n","# Call GPT-3 with the constructed prompt\n","summary = summarizer(prompt)\n","\n","# Print the summary\n","print(summary)\n","\n","with open(\"conversation.txt\", \"w\") as f:\n","    for message in debate_history:\n","        f.write(message + \"\\n\")\n","with open(\"conversation.txt\", \"r\") as f:\n","    content = f.read()\n","    print(content)\n","\n","import nltk\n","nltk.download('wordnet')\n","import random\n","from nltk.corpus import wordnet\n","from langchain.llms import OpenAI\n","\n","def rearrange_word_chunks(file_path):\n","    with open(file_path, 'r') as f:\n","        lines = f.readlines()\n","\n","    sentences = [line.strip() for line in lines if line.strip()]\n","    word_chunks = [word for sentence in sentences for word in sentence.split()]\n","    random.shuffle(word_chunks)\n","\n","    rearranged_file_path = 'rearranged_word_chunks.txt'\n","\n","    def free_associate(word):\n","        synonyms = []\n","        for syn in wordnet.synsets(word):\n","            for lemma in syn.lemmas():\n","                synonyms.append(lemma.name())\n","        if synonyms:\n","            return random.choice(synonyms)\n","        return word\n","\n","    with open(rearranged_file_path, 'w') as f:\n","        for word_chunk in word_chunks:\n","            f.write(word_chunk + ' ')\n","            f.write(free_associate(word_chunk) + ' ')\n","\n","    return rearranged_file_path\n","\n","rearranged_file_path = rearrange_word_chunks('conversation.txt')\n","\n","def create_story(prompt, max_tokens=2000):\n","    llm = OpenAI(temperature=1.0, max_tokens=max_tokens)\n","    response = llm(prompt)\n","    return response\n","\n","with open(rearranged_file_path, 'r') as f:\n","    rearranged_and_associated_words = f.read()\n","\n","story_prompt = (\n","    f\"Create a story using the following rearranged and free-associated words from a conversation:\\n\\n\"\n","    f\"{rearranged_and_associated_words}\\n\\n\"\n","    \"Story:\"\n",")\n","\n","story = create_story(story_prompt)\n","\n","with open('story.txt', 'w') as f:\n","    f.write(story)\n","\n","print (story)\n","\n","def find_solutions(userinput, story):\n","    llm = OpenAI(temperature=1.0)  # You can adjust the temperature value depending on the desired randomness\n","\n","    # Construct a prompt for GPT-3\n","    prompt = (\n","        f\"Given the following story generated from rearranged and free-associated words from a conversation:\\n\\n\"\n","        f\"{story}\\n\\n\"\n","        f\"Identify potential ethical implications or concerns to the following problem or topic: {userinput}\\n\\n\"\n","        \"Potential ethical concerns:\"\n","    )\n","\n","    # Call GPT-3 with the constructed prompt\n","    response = llm(prompt)\n","\n","    return response\n","\n","\n","# Read the content of the 'story.txt' file into the 'story' variable\n","with open('story.txt', 'r') as f:\n","    story = f.read()\n","\n","solutions_or_insights = find_solutions(userinput, story)\n","\n","print(solutions_or_insights)"]},{"cell_type":"code","source":[],"metadata":{"id":"iNoTHTTdyPyS"},"execution_count":null,"outputs":[]}]}